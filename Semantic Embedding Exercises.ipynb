{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Semantic Embeddings**\n",
        "## *Applications in Psychology and the Neurosciences*\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "C7KIzvCEvDtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Necessary Libraries\n",
        "from absl import logging\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "from gensim.models import KeyedVectors\n",
        "import gensim.downloader"
      ],
      "metadata": {
        "id": "ON3kSbcqvtjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove = gensim.downloader.load('glove-wiki-gigaword-300')\n",
        "word2vec = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "emdVTIZEVx6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show all available models in gensim-data\n",
        "print(list(gensim.downloader.info()['models'].keys()))"
      ],
      "metadata": {
        "id": "5yyP4rWcalIY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word Embeddings**"
      ],
      "metadata": {
        "id": "Gatix7cZvW-7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **GloVE Model**\n",
        "GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Let's download one of the pre-trained models. There are many to choose from, but let's use the one that was trained through Wikipedia."
      ],
      "metadata": {
        "id": "_HQ-Iz1dustE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import GloVe Model\n",
        "# glove = gensim.downloader.load('glove-wiki-gigaword-300')"
      ],
      "metadata": {
        "id": "q2GYxe2xU3p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example One**\n",
        "We can find a word in the GloVe dictionary are reveal its corresponding vector."
      ],
      "metadata": {
        "id": "j4ZoQk8M0YV4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove['listen']"
      ],
      "metadata": {
        "id": "E4y5GV-N0X6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "glove['hear']"
      ],
      "metadata": {
        "id": "vZhNjKQq05_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Your Turn**\n",
        "Using the same style as above, output the vector for two words of your choosing. Comment on how similar you think these words are based on the vector outputs."
      ],
      "metadata": {
        "id": "ImfwG0tg14jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code Space Provided\n"
      ],
      "metadata": {
        "id": "qZxQhLva3Ich"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example Two**\n",
        "We cannot determine how similar two words are solely by looking at their output vectors. We must have a way of quantifying the similarity. This is where dot product comes in.  Let's build this function together."
      ],
      "metadata": {
        "id": "1yD5CIes5bRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(word1, word2):\n",
        "  # First, lets find the embedding of each word\n",
        "  word1_embedding =\n",
        "  word2_embedding =\n",
        "\n",
        "  # Next, lets calculate the dot product using np.dot\n",
        "  dot_product =\n",
        "\n",
        "  # Let's not forget to scale the dot product. We can use the np.linalg.norm function to find the magnitude of the vector\n",
        "  cosine_similarity = dot_product /\n",
        "\n",
        "  return cosine_similarity"
      ],
      "metadata": {
        "id": "-IAAzjBC51FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's put this new function to use. How similar are those words?"
      ],
      "metadata": {
        "id": "UPMQWCyf7RlN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity('listen', 'hear')"
      ],
      "metadata": {
        "id": "vLeHZ03S7hd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, these two words have a cosine_similarity value above 0.8. This signifies that these words are quite similar."
      ],
      "metadata": {
        "id": "paMCSXnp7qY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Your Turn**\n",
        "Take the two words you used before and calculate their similarity. Are they more or less related than the example? Why do you think that is?"
      ],
      "metadata": {
        "id": "wJdg4vVq75eh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code space provided\n"
      ],
      "metadata": {
        "id": "SZGrx-AU7ptn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example Three**\n",
        "We can also use word embeddings to see which words are most similar to our word of choice. We can create a function which takes our word and looks within the dictionary to find which word has the highest cosine similarity value.\\\n",
        "This code is provided below."
      ],
      "metadata": {
        "id": "tHxEckbN8T1C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def most_similar(word, topn=5):\n",
        "    word_vector = glove[word]\n",
        "    similarities = np.dot(glove.vectors, word_vector)\n",
        "    most_similar_indices = np.argsort(similarities)[::-1][:topn+1]\n",
        "    most_similar_words = [glove.index_to_key[index] for index in most_similar_indices]\n",
        "\n",
        "    return most_similar_words[1:]\n"
      ],
      "metadata": {
        "id": "jWkZyf1_9BKO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the words we used before, let's see which words are the most similar."
      ],
      "metadata": {
        "id": "_J0bunvu9YN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar('listen')"
      ],
      "metadata": {
        "id": "bzO2z5mL9mfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "most_similar('hear')"
      ],
      "metadata": {
        "id": "9bX_TJUqIhbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Are these words what you expected?\n",
        "\n",
        "\\\n",
        "#### **Your turn**\n",
        "Use the function above to find which words are most closely correspond to the words of your choosing. Anything interesting?"
      ],
      "metadata": {
        "id": "97ReTxqOJyKV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code space provided\n"
      ],
      "metadata": {
        "id": "fzwDIwcGKR-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Word2vec**\n",
        "This is another model which transforms words into vectors. Let's compare to see if we get the same results as GloVe.\\\n",
        "Let's first load the pre-trained Word2vec embeddings. There are many to choose from, but let's use the one trained through Google News."
      ],
      "metadata": {
        "id": "hp2E6Zu7KX22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import word2vec model\n",
        "# word2vec = gensim.downloader.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "id": "cxozz-CSOZM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example Four**\n",
        "Just like we did with GloVe, let's check out the embeddings for some words.  Using the spece provided below, find the vectors for the same words you used in the previous exercise. At first glance, is it what you expected?"
      ],
      "metadata": {
        "id": "g62FS1U8csEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code space provided\n"
      ],
      "metadata": {
        "id": "UoY6EloTdD-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example Five**\n",
        "Let's quantify the similarity between these two words. You can create another function if you would like, but we can use a built-in function to speed things up. The code is provided below. \\\n",
        "Now compare this results with the cosine similarity value you obtained with GloVe. Do they match?"
      ],
      "metadata": {
        "id": "3v7d_8VwdWzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code space provided\n",
        "word2vec.similarity('word1', 'word2')"
      ],
      "metadata": {
        "id": "mMypMXgXdWTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example Six**\n",
        "Let's see if the most similar words match between the two models. Implement the similarity call below and comment on their differences."
      ],
      "metadata": {
        "id": "uqqbuFqefJTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code space provided"
      ],
      "metadata": {
        "id": "ViQ7iNMifbmc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example Seven**\n",
        "Let's look closer to analogies and see how word vectors can reveal some interesting patterns. Because vectors exist in a defined space, we can add and subtract them to produce new vectors.\n",
        "\n",
        "We'll use the example, *King is to man as Queen is to woman*.  Start by embedding the four words in the analogy."
      ],
      "metadata": {
        "id": "oYTJK-mSfo-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code space provided\n"
      ],
      "metadata": {
        "id": "u716YFvhirxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, let's subtract *King* from *man* and add *Queen*."
      ],
      "metadata": {
        "id": "wXTBpbBeiZaX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code space provided\n"
      ],
      "metadata": {
        "id": "CTQ120yFixKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can take this new vector and use try to find the corresponding word. The code is provided below.\\\n",
        "Did the model accurately predict the word? Compare between models."
      ],
      "metadata": {
        "id": "JVklNAUpi6bp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec.similar_by_vector('name of your new vector')"
      ],
      "metadata": {
        "id": "uazSM9wmnrSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Your Turn**\n",
        "Follow the same steps to complete a word analogy of your choosing."
      ],
      "metadata": {
        "id": "G4amGxKKolxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code space provided\n"
      ],
      "metadata": {
        "id": "ID9MkimPowBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Sentence Embeddings**\n",
        "### **Universal Sentence Encoder**\n",
        "The Universal Sentence Encoder goes beyond simple word embeddings by encoding longer pieces of text such as complete sentences and paragraphs. It not only captures semantic context, but also episodic context. Be careful though, the longer the text, the more diluted a single piece of information becomes."
      ],
      "metadata": {
        "id": "VB3TH3tBo-37"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import USE Model\n",
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\" #@param [\"https://tfhub.dev/google/universal-sentence-encoder/4\", \"https://tfhub.dev/google/universal-sentence-encoder-large/5\"]\n",
        "model = hub.load(module_url)\n",
        "\n",
        "def embed(input):\n",
        "  return model(input)"
      ],
      "metadata": {
        "id": "sdJnP65uvIZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example Eight**\n",
        "Let's start by embedding a simple sentence.  We can do this by using the embed function. Make sure you place your sentence sting within square brackets to avoid any errors."
      ],
      "metadata": {
        "id": "qt9XvhTUvmCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = 'We are all brilliant psychologists.'\n",
        "embed([sentence])"
      ],
      "metadata": {
        "id": "ga8RyA2ivzTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just like before, the model output a vector of numbers. Are there any evident difference between the GloVe and Word2Vec outputs?"
      ],
      "metadata": {
        "id": "ixktq3gUwY1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example Nine**\n",
        "A heatmap is a great way to show the relationship between vectors. The code is provided below.\\\n",
        "Your task is to create a list of five sentences and visualize their similarity using the heatmap."
      ],
      "metadata": {
        "id": "US6MpMB4wsku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Heatmap Function\n",
        "def plot_similarity(labels, features, rotation):\n",
        "  corr = np.inner(features, features)\n",
        "  sns.set(font_scale=1.2)\n",
        "  g = sns.heatmap(\n",
        "      corr,\n",
        "      xticklabels=labels,\n",
        "      yticklabels=labels,\n",
        "      vmin=0,\n",
        "      vmax=1,\n",
        "      cmap=\"Blues\")\n",
        "  g.set_xticklabels(labels, rotation=rotation)\n",
        "  g.set_title(\"Semantic Textual Similarity\")\n",
        "\n",
        "def run_and_plot(messages_):\n",
        "  message_embeddings_ = embed(messages_)\n",
        "  plot_similarity(messages_, message_embeddings_, 90)"
      ],
      "metadata": {
        "id": "eXEEIbygwng1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code space provided\n",
        "sentences = ['sentence 1',\n",
        "             'sentence 2']\n",
        "\n",
        "run_and_plot(sentences)"
      ],
      "metadata": {
        "id": "MRPhW9SpxTT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example Ten**\n",
        "The previous examples shows the similarity between sentences of the same sample. We can also use the USE and heatmap visualization to show the similarity of two different sentence sets. Let's make a quick adjustment to the function."
      ],
      "metadata": {
        "id": "X81TqLxryEUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_similarity_new(labels1, labels2, features1, features2, rotation):\n",
        "  corr = np.inner(features1, features2)\n",
        "  sns.set(font_scale=1.2)\n",
        "  g = sns.heatmap(\n",
        "      corr,\n",
        "      xticklabels=labels2,\n",
        "      yticklabels=labels1,\n",
        "      vmin=0,\n",
        "      vmax=1,\n",
        "      cmap=\"Blues\")\n",
        "  g.set_xticklabels(labels2, rotation=rotation)\n",
        "  g.set_title(\"Semantic Textual Similarity\")\n",
        "\n",
        "def run_and_plot_new(messages1_, messages2_):\n",
        "  message_embeddings1_ = embed(messages1_)\n",
        "  message_embeddings2_ = embed(messages2_)\n",
        "  plot_similarity_new(messages1_, messages2_, message_embeddings1_, message_embeddings2_, 90)"
      ],
      "metadata": {
        "id": "AFvKddBLyhpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, create a new list of 5 sentences and we will visualize the similarity between the set of sentences from the previous examples."
      ],
      "metadata": {
        "id": "2RwjCzwnzKS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_sentences = ['sentence 1',\n",
        "                 'sentence 2',\n",
        "                 'sentence 3']\n",
        "\n",
        "run_and_plot_new(sentences, new_sentences)"
      ],
      "metadata": {
        "id": "JI57hWZdzVZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Example Eleven**\n",
        "Surprisingly, there is a way that we can embed sentences using word embedding models.  But does it do as good a job as the USE? Let's investigate."
      ],
      "metadata": {
        "id": "ST9Ug-K-zkdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string_1 = 'I attend Raptors basketball games at least once a year.'\n",
        "string_2 = 'I came all the way to Toronto to see the Raptors play basketball.'"
      ],
      "metadata": {
        "id": "Iypr-gFM4a1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split each sentence into individual words\n",
        "tokens_1 = string_1.lower().split()\n",
        "tokens_2 = string_2.lower().split()\n",
        "\n",
        "print(tokens_1)\n",
        "\n",
        "# Create a matrix with the embedding of each word\n",
        "matrix_1 = [word2vec[token] for token in tokens_1 if token in word2vec]\n",
        "matrix_2 = [word2vec[token] for token in tokens_2 if token in word2vec]\n",
        "\n",
        "# Calculate of the word vectors\n",
        "vector_1 = np.mean(matrix_1, axis=0)\n",
        "vector_2 = np.mean(matrix_2, axis=0)\n",
        "\n",
        "# Calculate the cosine similarity\n",
        "cosine_similarity = np.dot(vector_1, vector_2) / (np.linalg.norm(vector_1) * np.linalg.norm(vector_2))\n",
        "cosine_similarity\n"
      ],
      "metadata": {
        "id": "uc4vxEoF0Cbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is a function that accomplishes the same task."
      ],
      "metadata": {
        "id": "n4Nl_rwc4k4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vector1, vector2):\n",
        "  cosine_similarity = np.inner(vector1, vector2) / (np.linalg.norm(vector_1) * np.linalg.norm(vector_2))\n",
        "\n",
        "  return cosine_similarity\n",
        "\n",
        "def sentence_word_embedding(sentence1, sentence2, model):\n",
        "  # Split each sentence into individual words\n",
        "  tokens_1 = sentence1.lower().split()\n",
        "  tokens_2 = sentence2.lower().split()\n",
        "\n",
        "  # Create a matrix with the embedding of each word\n",
        "  matrix_1 = [word2vec[token] for token in tokens_1 if token in word2vec]\n",
        "  matrix_2 = [word2vec[token] for token in tokens_2 if token in word2vec]\n",
        "\n",
        "  # Calculate of the word vectors\n",
        "  vector_1 = np.mean(matrix_1, axis=0)\n",
        "  vector_2 = np.mean(matrix_2, axis=0)\n",
        "\n",
        "  # Calculate the cosine similarity\n",
        "  cosine_similarity(vector_1, vector_2)\n",
        "\n",
        "  return cosine_similarity"
      ],
      "metadata": {
        "id": "cuXJScHH4iL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's see whether the relationship between those two sentences is consistent if we use the USE. Embed the two sentences below and then input the vectors into the cosine_similarity function."
      ],
      "metadata": {
        "id": "pdEJs_Ny5FXD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_1 =\n",
        "embedding_2 =\n",
        "\n",
        "cosine_similarity(embedding_1, embedding_2)\n"
      ],
      "metadata": {
        "id": "h1yyVsz85EVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What can you observe from these results? Are there any evident differences between those two results?\n",
        "#### **Your turn**\n",
        "Create two sentences of your own and see whether the word embeddings accurately capture semantic similarity."
      ],
      "metadata": {
        "id": "N4Am0coU67-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code space provided\n"
      ],
      "metadata": {
        "id": "hieHYG3y76lH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Recall Task**\n",
        "Let's test whether the USE can be a useful tool to measure participants recall ability. After hearing the short story presented, recall the story by typing below."
      ],
      "metadata": {
        "id": "NwBB5kEk73y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "story_recall = 'Oliver was a boy that lived in a busy city. Hw was also intrigued by the human mind. He spend countless hours devouring books on psychology and attending lectures. He pursued his degree at a university and eventually opened up his own practice. He helped clients face their fear and heal from trauma. He helped a lot of people. He became a beacon of help for everyone around him.'"
      ],
      "metadata": {
        "id": "eK88sdbc8Y04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story = '''\n",
        "In a bustling city filled with dreams and ambitions, there lived a young boy named Oliver. From a tender age, Oliver had always possessed a keen sense of empathy and a deep curiosity about the human mind.\n",
        "As he grew older, Oliver found himself captivated by the stories of people around him, their joys, sorrows, and the complexities of their emotions. He yearned to unravel the mysteries of the human psyche and provide solace to those in need.\n",
        "Driven by his passion, Oliver spent countless hours devouring books on psychology, attending lectures, and engaging in thought-provoking conversations with mentors and experts in the field. Each encounter only fueled his desire to make a difference in the lives of others.\n",
        "Through his own introspection, Oliver discovered that his purpose lay in becoming a psychologist—a guide who could navigate the intricate pathways of the mind, helping individuals navigate their own journeys of self-discovery and healing.\n",
        "With unwavering determination, Oliver pursued his education, studying psychology at a prestigious university. He immersed himself in the realms of cognitive science, behaviour analysis, and therapeutic techniques, honing his skills and expanding his understanding of the human psyche.\n",
        "As Oliver completed his studies and obtained his degree, he embarked on his professional journey, opening his own practice. People from all walks of life sought his guidance, seeking solace in his empathetic presence and skilled guidance.\n",
        "Oliver's compassion, coupled with his expertise, allowed him to make a profound impact on his clients' lives. He helped them confront their fears, heal from past traumas, and navigate the complexities of relationships. Through therapy, he empowered individuals to unlock their potential and find inner peace.\n",
        "Years passed, and Oliver's reputation as a compassionate and insightful psychologist spread far and wide. His dedication to his craft and genuine concern for his clients earned him the respect and admiration of his peers and the community.\n",
        "But Oliver's true measure of success was the transformation he witnessed in the lives of those he helped. Each person who walked through his door carried a unique story, and Oliver considered it an honor to be a part of their journey towards self-discovery and growth.\n",
        "And so, Oliver, the boy who dreamt of understanding the human mind, became a beacon of hope and healing for countless individuals. His unwavering commitment to his passion brought light to the lives of others and shaped his own destiny as a respected and beloved psychologist.\n",
        "'''\n",
        "\n",
        "sentences = sent_tokenize(story)\n",
        "print(sentences)\n"
      ],
      "metadata": {
        "id": "NNW9xXHd-01M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make sure all words are lowercase and split recall into individual sentences.\n",
        "recall_sentences = sent_tokenize(story_recall.lower())\n",
        "story_sentences = sent_tokenize(story.lower())\n",
        "\n",
        "run_and_plot_new(story_sentences, recall_sentences)"
      ],
      "metadata": {
        "id": "ONoq6I6q9i9x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}